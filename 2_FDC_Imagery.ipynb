{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Interactive stream flow curation curve (FDC) using HRS stream gauges (automated retrieval from BoM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook creates an interactive flow duration curve (FDC) that is used to return Earth Observation images.\n",
    "\n",
    "Daily streamflow information and stream gauge coordinates are retrieved directly from the Bureau of Meteorology (BoM) Hydrologic Reference Stations (HRS) website, http://www.bom.gov.au/water/hrs/. Streamflow data is used to calculate the percentage exceedance statistic (the percentage of time that the streamflow value is equalled or exceeded by all other streamflow values within the data set). \n",
    "\n",
    "The date of streamflow measurement and date of available Landsat satellite products are matched, for the location of the gauging station. A FDC plot is created that enables the user to click on a streamflow event and return the closest satellite image. Images are returned for a small area and then a much larger area surrounding the stream gauging location. There are options to save imagery in JPG or netcdf format.\n",
    "\n",
    "\"###\" indicates fields that require user modification.\n",
    "\n",
    "\"##\" indicates fields that may require user modification, e.g. graphic edits/preferences.\n",
    "\n",
    "\"#\" indicates a cell title or description of code. No modification is required.\n",
    "\n",
    "Code written in May 2017 by Erin Telfer with support from Leo Lymburner, Damien Ayers and Biswajit Bala. \n",
    "\n",
    "The notebook was completed as a graduate program project at Geoscience Australia. If you find an error or if you have any suggestions, please contact erin.telfer@ga.gov.au. Alternatively, please contact leo.lymburner@ga.gov.au."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-06-09T11:09:06.502514",
     "start_time": "2016-06-09T11:09:04.805211"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "#Import libraries\n",
    "%pylab notebook\n",
    "\n",
    "import datacube\n",
    "from datacube.storage import masking\n",
    "from datacube.storage.masking import mask_to_dict\n",
    "from datacube.storage.storage import write_dataset_to_netcdf\n",
    "from datacube.helpers import ga_pq_fuser\n",
    "\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "import numpy as np\n",
    "import csv\n",
    "import os\n",
    "import datetime\n",
    "\n",
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "from matplotlib import pyplot as plt\n",
    "import matplotlib.dates\n",
    "from IPython.display import display\n",
    "import ipywidgets as widgets\n",
    "\n",
    "import rasterio\n",
    "import urllib\n",
    "from pyproj import Proj, transform\n",
    "from dateutil import tz\n",
    "from_zone = tz.tzutc()\n",
    "to_zone = tz.tzlocal()\n",
    "dc = datacube.Datacube(app='dc-show changes in annual mean NDVI values')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieve the stream gauge data and coordinates from the BoM website\n",
    "The URL for the data is set using the gauge_of_interest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Specify stream gauge of interest.\n",
    "\n",
    "###User modification: Enter the ID code for the gauge of interest. ID code can be viewed on \n",
    "###http://www.bom.gov.au/water/hrs/ e.g. for 'Diamantina River at Birdsville' the ID code is 'A0020101'\n",
    "\n",
    "gauge_of_interest= 'G9030250'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             Date     Flow (ML) Bureau QCode\n",
      "0      1966-08-13     10.290800            E\n",
      "1      1966-08-14     10.009100            E\n",
      "2      1966-08-15      9.956920            E\n",
      "3      1966-08-16      9.579110            E\n",
      "4      1966-08-17      9.292170            E\n",
      "5      1966-08-18      9.196090            E\n",
      "6      1966-08-19      9.297820            E\n",
      "7      1966-08-20      9.863880            E\n",
      "8      1966-08-21      6.381870            E\n",
      "9      1966-08-22      4.254020            E\n",
      "10     1966-08-23      3.432010            E\n",
      "11     1966-08-24      3.303020            E\n",
      "12     1966-08-25      3.264020            E\n",
      "13     1966-08-26      3.299020            E\n",
      "14     1966-08-27      3.275020            E\n",
      "15     1966-08-28      3.240020            E\n",
      "16     1966-08-29      3.239020            E\n",
      "17     1966-08-30      3.120980            E\n",
      "18     1966-08-31      2.600010            E\n",
      "19     1966-09-01      1.805000            E\n",
      "20     1966-09-02      1.603010            E\n",
      "21     1966-09-03      1.446020            E\n",
      "22     1966-09-04      1.173990            E\n",
      "23     1966-09-05      0.737009            E\n",
      "24     1966-09-06      0.496020            E\n",
      "25     1966-09-07      0.362000            E\n",
      "26     1966-09-08      0.220998            E\n",
      "27     1966-09-09      0.095000            E\n",
      "28     1966-09-10      0.033000            E\n",
      "29     1966-09-11      0.000000            E\n",
      "...           ...           ...          ...\n",
      "17643  2014-12-02    158.923000            E\n",
      "17644  2014-12-03    150.693000            E\n",
      "17645  2014-12-04    158.023000            E\n",
      "17646  2014-12-05    179.252000            E\n",
      "17647  2014-12-06    212.242000            E\n",
      "17648  2014-12-07    280.126000            E\n",
      "17649  2014-12-08    287.208000            E\n",
      "17650  2014-12-09    361.783000            E\n",
      "17651  2014-12-10    383.925000            E\n",
      "17652  2014-12-11    389.403000            E\n",
      "17653  2014-12-12    372.139000            E\n",
      "17654  2014-12-13    631.840000            E\n",
      "17655  2014-12-14   1183.420000            E\n",
      "17656  2014-12-15   1650.920000            E\n",
      "17657  2014-12-16   6151.880000            E\n",
      "17658  2014-12-17   2879.860000            E\n",
      "17659  2014-12-18   1265.160000            E\n",
      "17660  2014-12-19    737.835000            E\n",
      "17661  2014-12-20    520.758000            E\n",
      "17662  2014-12-21    413.107000            E\n",
      "17663  2014-12-22    321.198000            E\n",
      "17664  2014-12-23    413.550000            E\n",
      "17665  2014-12-24   1253.070000            E\n",
      "17666  2014-12-25   1968.820000            E\n",
      "17667  2014-12-26   2305.540000            E\n",
      "17668  2014-12-27   2920.950000            E\n",
      "17669  2014-12-28   4038.590000            E\n",
      "17670  2014-12-29  10443.400000            E\n",
      "17671  2014-12-30  16432.300000            E\n",
      "17672  2014-12-31  23661.900000            E\n",
      "\n",
      "[17673 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "#Url is used to retrieve daily streamflow data for the gauge_of_interest    \n",
    "url = 'http://www.bom.gov.au/water/hrs/content/data/'+gauge_of_interest+'/'+gauge_of_interest+'_daily_ts.csv'\n",
    "gaugedata = pd.read_csv(url, comment='#')\n",
    "print (gaugedata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Geographic: 134.4197, -14.6953\n",
      "Australian Albers: 263136.9744314623, -1560666.1358515057\n"
     ]
    }
   ],
   "source": [
    "#Url is used to retrieve stream gauge location coordinates. Coordinates are reprojected to Australian Albers    \n",
    "\n",
    "#Search url to find coordinates \n",
    "coord_txt = urllib.request.urlopen(url).read()\n",
    "coord_txt = str(coord_txt)\n",
    "sg_lon = coord_txt.split('\"Location:\", ')[1].split(',\"degrees E\",')[0]\n",
    "sg_lon=float(sg_lon)\n",
    "sg_lat = coord_txt.split(',\"degrees E\", ')[1].split(',\"degrees S\"')[0]\n",
    "sg_lat= \"-\"+sg_lat\n",
    "sg_lat=float(sg_lat)\n",
    "\n",
    "#Reproject to Australian Albers\n",
    "inProj = Proj(init='EPSG:4326')\n",
    "outProj = Proj(init='EPSG:3577')\n",
    "sg_x,sg_y = transform(inProj,outProj,sg_lon,sg_lat)\n",
    "\n",
    "print (\"Geographic: \" + str(sg_lon)+', '+ str(sg_lat))\n",
    "print (\"Australian Albers: \"+ str(sg_x)+', '+str(sg_y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Complete Datacube query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Spatiotemporal range and wavelengths/band of interest are defined\n",
    "\n",
    "#Temporal range is defined\n",
    "start_of_epoch = '1987-01-01'\n",
    "end_of_epoch =  '2014-12-31'\n",
    "\n",
    "#Wavelengths/bands of interest are defined\n",
    "bands_of_interest = ['green',\n",
    "                     'red', \n",
    "                     'nir',\n",
    "                     'swir1']\n",
    "\n",
    "#Landsat sensors of interest are defined\n",
    "sensors = ['ls8',\n",
    "    'ls7',\n",
    "    'ls5' ] \n",
    "\n",
    "#Create bounding box around the location of the stream gauge\n",
    "lat_max = sg_lat + 0.05\n",
    "lat_min = sg_lat - 0.05\n",
    "lon_max = sg_lon + 0.05\n",
    "lon_min = sg_lon - 0.05\n",
    "\n",
    "#Create query\n",
    "query = {'time': (start_of_epoch, end_of_epoch)}\n",
    "query['x'] = (lon_min, lon_max)\n",
    "query['y'] = (lat_max, lat_min)\n",
    "query['crs'] = 'EPSG:4326'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Create cloud mask. This will define which pixel quality (PQ) artefacts are removed from the results.\n",
    "\n",
    "mask_components = {'cloud_acca':'no_cloud',\n",
    "'cloud_shadow_acca' :'no_cloud_shadow',\n",
    "'cloud_shadow_fmask' : 'no_cloud_shadow',\n",
    "'cloud_fmask' :'no_cloud',\n",
    "'blue_saturated' : False,\n",
    "'green_saturated' : False,\n",
    "'red_saturated' : False,\n",
    "'nir_saturated' : False,\n",
    "'swir1_saturated' : False,\n",
    "'swir2_saturated' : False,\n",
    "'contiguous':True}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Complete Datacube extraction\n",
    "The extracted data is first filtered using the criteria in \"mask_components\". The cloudiness of the scenes is then tested, and any scenes that do not meet the given \"cloud_free_threshold\" are discarded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded ls8\n",
      "loaded ls7\n",
      "loaded ls5\n",
      "complete\n"
     ]
    }
   ],
   "source": [
    "#Retrieve the data for each Landsat sensor\n",
    "\n",
    "sensor_clean = {}\n",
    "cloud_free_threshold = 0.99  ##User modification: set cloud threshold. Default value is \"0.90\" or >90% image and <10% cloud cover\n",
    "                        ###Scenes will be discarded that have less than the cloud threshold worth of image.\n",
    "\n",
    "for sensor in sensors:\n",
    "    #Load the NBAR and corresponding PQ\n",
    "    sensor_nbar = dc.load(product= sensor+'_nbar_albers', group_by='solar_day', \n",
    "                          measurements = bands_of_interest,  **query)\n",
    "    sensor_pq = dc.load(product= sensor+'_pq_albers', group_by='solar_day', \n",
    "                        fuse_func=ga_pq_fuser, **query)\n",
    "    \n",
    "    #Retrieve the projection information before masking/sorting\n",
    "    crs = sensor_nbar.crs\n",
    "    crswkt = sensor_nbar.crs.wkt\n",
    "    affine = sensor_nbar.affine\n",
    "    \n",
    "    #Ensure there's PQ to go with the NBAR\n",
    "    sensor_nbar = sensor_nbar.sel(time = sensor_pq.time)\n",
    "    \n",
    "    #Apply the PQ masks to the NBAR\n",
    "    quality_mask = masking.make_mask(sensor_pq, **mask_components)\n",
    "    good_data = quality_mask.pixelquality.loc[start_of_epoch:end_of_epoch]\n",
    "    sensor_nbar2 = sensor_nbar.where(good_data)\n",
    "    \n",
    "    #Calculate the percentage cloud free for each scene\n",
    "    cloud_free = masking.make_mask(sensor_pq, cloud_acca='no_cloud', cloud_fmask='no_cloud', \n",
    "                                   contiguous=True).pixelquality\n",
    "    mostly_cloud_free = cloud_free.mean(dim=('x','y')) >= cloud_free_threshold\n",
    "        \n",
    "    #Discard data that does not meet the cloud_free_threshold\n",
    "    mostly_good = sensor_nbar2.where(mostly_cloud_free).dropna(dim='time', how='all')\n",
    "    mostly_good['product'] = ('time', numpy.repeat(sensor, mostly_good.time.size))    \n",
    "    sensor_clean[sensor] = mostly_good\n",
    "\n",
    "    print('loaded %s' % sensor) \n",
    "    \n",
    "\n",
    "print ('complete')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ls5': <xarray.Dataset>\n",
       " Dimensions:  (time: 219, x: 445, y: 447)\n",
       " Coordinates:\n",
       "   * time     (time) datetime64[ns] 1987-05-25T00:29:06.500000 ...\n",
       "   * y        (y) float64 -1.555e+06 -1.555e+06 -1.555e+06 -1.555e+06 ...\n",
       "   * x        (x) float64 2.576e+05 2.576e+05 2.576e+05 2.577e+05 2.577e+05 ...\n",
       " Data variables:\n",
       "     green    (time, y, x) float64 797.0 750.0 797.0 844.0 797.0 891.0 844.0 ...\n",
       "     red      (time, y, x) float64 880.0 837.0 922.0 964.0 1.006e+03 ...\n",
       "     nir      (time, y, x) float64 2.112e+03 2.164e+03 2.217e+03 2.269e+03 ...\n",
       "     swir1    (time, y, x) float64 2.472e+03 2.472e+03 2.472e+03 2.509e+03 ...\n",
       "     product  (time) <U3 'ls5' 'ls5' 'ls5' 'ls5' 'ls5' 'ls5' 'ls5' 'ls5' ...\n",
       " Attributes:\n",
       "     crs:      EPSG:3577, 'ls7': <xarray.Dataset>\n",
       " Dimensions:  (time: 39, x: 445, y: 447)\n",
       " Coordinates:\n",
       "   * time     (time) datetime64[ns] 1999-07-21T00:57:50 ...\n",
       "   * y        (y) float64 -1.555e+06 -1.555e+06 -1.555e+06 -1.555e+06 ...\n",
       "   * x        (x) float64 2.576e+05 2.576e+05 2.576e+05 2.577e+05 2.577e+05 ...\n",
       " Data variables:\n",
       "     green    (time, y, x) float64 680.0 629.0 655.0 681.0 706.0 757.0 783.0 ...\n",
       "     red      (time, y, x) float64 822.0 822.0 800.0 890.0 958.0 1.003e+03 ...\n",
       "     nir      (time, y, x) float64 1.856e+03 1.823e+03 1.856e+03 1.922e+03 ...\n",
       "     swir1    (time, y, x) float64 2.464e+03 2.345e+03 2.375e+03 2.494e+03 ...\n",
       "     product  (time) <U3 'ls7' 'ls7' 'ls7' 'ls7' 'ls7' 'ls7' 'ls7' 'ls7' ...\n",
       " Attributes:\n",
       "     crs:      EPSG:3577, 'ls8': <xarray.Dataset>\n",
       " Dimensions:  (time: 20, x: 445, y: 447)\n",
       " Coordinates:\n",
       "   * time     (time) datetime64[ns] 2013-04-14T01:06:51.500000 ...\n",
       "   * y        (y) float64 -1.555e+06 -1.555e+06 -1.555e+06 -1.555e+06 ...\n",
       "   * x        (x) float64 2.576e+05 2.576e+05 2.576e+05 2.577e+05 2.577e+05 ...\n",
       " Data variables:\n",
       "     green    (time, y, x) float64 720.0 696.0 719.0 761.0 795.0 775.0 753.0 ...\n",
       "     red      (time, y, x) float64 790.0 739.0 770.0 859.0 922.0 905.0 849.0 ...\n",
       "     nir      (time, y, x) float64 1.842e+03 1.942e+03 1.987e+03 2.035e+03 ...\n",
       "     swir1    (time, y, x) float64 2.192e+03 2.044e+03 2.13e+03 2.309e+03 ...\n",
       "     product  (time) <U3 'ls8' 'ls8' 'ls8' 'ls8' 'ls8' 'ls8' 'ls8' 'ls8' ...\n",
       " Attributes:\n",
       "     crs:      EPSG:3577}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check the output\n",
    "sensor_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Concatenate (join) the data from the different sensors together and sort so that observations are sorted \n",
    "#by time rather than sensor\n",
    "\n",
    "nbar_clean = xr.concat(sensor_clean.values(), dim='time')\n",
    "time_sorted = nbar_clean.time.argsort()\n",
    "nbar_clean = nbar_clean.isel(time=time_sorted)\n",
    "nbar_clean.attrs['crs'] = crs\n",
    "nbar_clean.attrs['affin|e'] = affine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<xarray.Dataset>\n",
       "Dimensions:  (time: 278, x: 445, y: 447)\n",
       "Coordinates:\n",
       "  * y        (y) float64 -1.555e+06 -1.555e+06 -1.555e+06 -1.555e+06 ...\n",
       "  * x        (x) float64 2.576e+05 2.576e+05 2.576e+05 2.577e+05 2.577e+05 ...\n",
       "  * time     (time) datetime64[ns] 1987-05-25T00:29:06.500000 ...\n",
       "Data variables:\n",
       "    green    (time, y, x) float64 797.0 750.0 797.0 844.0 797.0 891.0 844.0 ...\n",
       "    red      (time, y, x) float64 880.0 837.0 922.0 964.0 1.006e+03 ...\n",
       "    nir      (time, y, x) float64 2.112e+03 2.164e+03 2.217e+03 2.269e+03 ...\n",
       "    swir1    (time, y, x) float64 2.472e+03 2.472e+03 2.472e+03 2.509e+03 ...\n",
       "    product  (time) <U3 'ls5' 'ls5' 'ls5' 'ls5' 'ls5' 'ls5' 'ls5' 'ls5' ...\n",
       "Attributes:\n",
       "    crs:      EPSG:3577\n",
       "    affin|e:  | 25.00, 0.00, 257575.00|\\n| 0.00,-25.00,-1555075.00|\\n| 0.00, ..."
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check that the concatenation worked\n",
    "nbar_clean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Process stream gauge information\n",
    "Calculate the percentiles of stream flow, and sort the dataframe according to date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Calculate \"percentage exceedance\" (perexc) for stream flow values\n",
    "\n",
    "all_data = gaugedata #Import streamflow data from the gauge of interest\n",
    "all_data= all_data.rename(columns={'Flow (ML)':'flow', 'Date':'date'})  #Rename flow and date columns\n",
    "all_data = all_data.sort_values('flow', ascending=[False]) #Sort data by flow value\n",
    "all_data['rank'] = np.arange(len(all_data)) + 1 #Create rank column and values\n",
    "all_data['perexc'] = 100*(all_data['rank'])/(len(all_data)+1) #Calculate probability of each rank\n",
    "all_data= all_data.sort_values(['date']) #Sort data by date\n",
    "all_data=all_data.drop(all_data.columns[[2]], axis=1) #Remove \"Bureau QCode\" column\n",
    "all_data['date']=pd.to_datetime(all_data['date'], format='%Y/%m/%d %H:%M:%S') #Change datetime format\n",
    "all_data.date = all_data.date.map(lambda t: t.strftime('%Y-%m-%d')) #Change datetime format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Return just the time and sensor product information from the Datacube extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "product_time = nbar_clean[['time', 'product']].to_dataframe() #Add time and product to dataframe\n",
    "product_time.index = product_time.index + pd.Timedelta(hours=10) #Roughly convert to local time\n",
    "product_time.index = product_time.index.map(lambda t: t.strftime('%Y-%m-%d')) #Remove Hours/Minutes Seconds by formatting into a string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Match the date of stream flow data to the date where satellite information exists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "subset_data = pd.merge(all_data, product_time, left_on= 'date', \n",
    "                       right_index=True, how='inner') #Match dates and merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "subset_data['date']=pd.to_datetime(subset_data['date'], format='%Y/%m/%d %H:%M:%S') # back to datetime format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create interactive flow duration curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Prepare flow and percentage exceedance variables for plotting on FDC\n",
    "\n",
    "#Prepare all data\n",
    "sorted_a_flow=sorted(all_data.flow, reverse=True)\n",
    "sorted_a_pe=sorted(all_data.perexc)\n",
    "\n",
    "#Prepare the matched subset data\n",
    "sorted_s_flow=sorted(subset_data.flow, reverse=True)\n",
    "sorted_s_pe=sorted(subset_data.perexc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Ceate interactive flow duration curve\n",
    "\n",
    "#create widget that enables interaction with plot\n",
    "w = widgets.HTML(\"Click on a point on the curve to display the satellite image\")\n",
    "def callback(event):\n",
    "    global discharge_int, perexc_int, devent\n",
    "    devent = event\n",
    "    discharge_int = event.ydata\n",
    "    perexc_int = event.xdata\n",
    "    discharge_int = discharge_int.astype(datetime64[D])\n",
    "    w.value = 'time_int: {}'.format(time_int)\n",
    "\n",
    "\n",
    "#Set up plot\n",
    "fig = plt.figure(figsize=(10,10)) #Edit size of plot ##User should format as required\n",
    "fig.canvas.mpl_connect('button_press_event', callback) #Plot setup\n",
    "plt.title('Interactive flow duration curve: '+gauge_of_interest) #Plot title ##User should format if required\n",
    "display(w) #Plot setup\n",
    "pyplot.yscale('log') #set up Y axis as a log scale\n",
    "plt.subplots_adjust(left=0.10, right=0.96, top=0.95, bottom=0.12) #Set border dimensions  ##User should format if required\n",
    "fig.patch.set_facecolor('white')  #Make border white ##User should format if required\n",
    "fig.patch.set_alpha(0.99)  #Make border white ##User should format if required\n",
    "plt.grid(True) #Add gridlines to the figure\n",
    "\n",
    "#create plot of percent exceedance\n",
    "plt.plot(sorted_a_pe,sorted_a_flow,'o',label= 'All discharge values')  #plot all discharge values ##User should format series if required\n",
    "plt.plot(sorted_s_pe,sorted_s_flow,'ro',label='Discharge values with suitable satellite imagery') #plot matched discharge values ##User should format series if required\n",
    "\n",
    "#axis and legend details\n",
    "plt.axis([-5, 105, 0.00001, 10000000]) ###User modification: set axis discharge values appropriate for streamflow range\n",
    "plt.xticks(rotation=45,size=14) #Rotate and format size of date labels ##User should format if required\n",
    "plt.yticks(size=14) #Format size of date labels ##User should format if required\n",
    "plt.ylabel('Discharge (m$^3$ day$^{-1}$)', size=14) #Set Y label\n",
    "plt.xlabel('Percentage of time equalled or exceeded (%)', size=14) #Set X label\n",
    "plt.legend(edgecolor ='none', ncol=2, loc=9, fontsize=12) #Set legend location on plot ##User should format if required\n",
    "\n",
    "plt.show() #Show plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# #Save figure\n",
    "# ###User modification: edit directory to save figure\n",
    "# %cd /g/data/r78/ext547/Output/FDC/\n",
    "# plt.savefig('FDC_'+gauge_of_interest+'_withsat.jpg')  ##User should format file name if required"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Show values for click on interactive graph\n",
    "print ('Clicked discharge value: ' + str(discharge_int) + ' m3')\n",
    "print ('Clicked percentage exceedance value: ' + str(perexc_int) +' %')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Define the closest real value from the clicked point on the interactive graph. For example, \n",
    "# find the exact date of the satellite pass or the exact discharge or flow exceedance value \n",
    "# and then format to use as title for images\n",
    "\n",
    "#Date\n",
    "time_slice=subset_data.iloc[(subset_data.perexc - perexc_int).abs().argsort()[0:1]].date #Find difference between clicked and actual perexc value, sort and return date value\n",
    "time_slice= (list(time_slice)[0]) #Specify date value with the smallest difference between clicked and actual value\n",
    "time_slice= str(time_slice) #Convert to string \n",
    "time_slice=datetime.datetime.strptime(time_slice,'%Y-%m-%d %H:%M:%S') #Convert to datetime\n",
    "time_slice_actual=time_slice  #Make new date variable\n",
    "time_slice_t1=time_slice_actual + datetime.timedelta(days=-2) #Make date variable two days before actual date \n",
    "time_slice_t2=time_slice_actual + datetime.timedelta(days=2) #Make date variable two days after actual date \n",
    "\n",
    "#Discharge \n",
    "discharge_title=subset_data.iloc[(subset_data.perexc - perexc_int).abs().argsort()[:1]].flow #Find difference between clicked and actual perexc value, sort and return flow value\n",
    "discharge_title2= float(discharge_title) #Convert to float\n",
    "discharge_title2=str(\"{0:.2f}\".format(discharge_title2)) #Convert to str and reformat\n",
    "\n",
    "#Percentage exceedance\n",
    "perexc_title=subset_data.iloc[(subset_data.perexc - perexc_int).abs().argsort()[:1]].perexc #Find difference between clicked and actual perexc value, sort and return perexc value\n",
    "perexc_title2= float(perexc_title) #Convert to float\n",
    "perexc_title2=str(\"{0:.2f}\".format(perexc_title2))  #Convert to str and reformat\n",
    "\n",
    "#Satellite\n",
    "satellite_type=subset_data.iloc[(subset_data.perexc - perexc_int).abs().argsort()[:1]]  #Find difference between clicked and actual perexc value and sort\n",
    "satellite_type=satellite_type['product'] #Specify product type\n",
    "satellite_type= (list(satellite_type)[0]) #Specify product type\n",
    "satellite_type= str(satellite_type)+'_nbar_albers' #reformat\n",
    "\n",
    "\n",
    "print ('Actual observation date: ' +str(time_slice_actual))\n",
    "print ('Discharge: ' +str(discharge_title2) +' m3')\n",
    "print ('Percent exceedance: '+ str(perexc_title2) + '%')\n",
    "print ('Product: '+ str(satellite_type)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#create FDC plot showing date of interest highlighted\n",
    "\n",
    "#Setup plot\n",
    "fig = plt.figure(figsize=(10,10)) #Edit size of plot ##User should format as required\n",
    "fig.canvas.mpl_connect('button_press_event', callback) #Plot setup\n",
    "plt.title('Interactive flow duration curve: '+gauge_of_interest, fontsize=14) #Plot title ##User should format if required\n",
    "display(w) #Plot setup\n",
    "pyplot.yscale('log') #set up Y axis as a log scale\n",
    "plt.subplots_adjust(left=0.10, right=0.96, top=0.95, bottom=0.12) #Set border dimensions  ##User should format if required\n",
    "fig.patch.set_facecolor('white')  #Make border white ##User should format if required\n",
    "fig.patch.set_alpha(0.99)  #Make border white ##User should format if required\n",
    "plt.grid(True) #Add gridlines to the figure\n",
    "\n",
    "#create plot of percent exceedance\n",
    "plt.plot(sorted_a_pe,sorted_a_flow,'o',label= 'All discharge values')  #plot all discharge values ##User should format series if required\n",
    "\n",
    "plt.plot(sorted_s_pe,sorted_s_flow,'ro',label='Discharge values with suitable satellite imagery') #plot matched discharge values ##User should format series if required\n",
    "\n",
    "plt.plot(perexc_title2,discharge_title2, 'ko', label='Event of interest',ms=10) #plot event of interest ##User should format series if required\n",
    "\n",
    "#axis and legend details\n",
    "plt.axis([-5, 105, 0.00001, 10000000]) ##User modification: set axis discharge values appropriate for streamflow range\n",
    "plt.xticks(rotation=45,size=14) #Rotate and format size of date labels ##User should format if required\n",
    "plt.yticks(size=14) #Format size of date labels ##User should format if required\n",
    "plt.ylabel('Discharge (m$^3$ day$^{-1}$)', size=14) #Set Y label\n",
    "plt.xlabel('Percentage of time equalled or exceeded (%)', size=14) #Set X label\n",
    "plt.legend(edgecolor ='none', ncol=1, loc=9, fontsize=14) #Set legend location on plot ##User should format if required\n",
    "\n",
    "plt.show() #Show plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Save figure\n",
    "###User modification: edit directory to save figure\n",
    "%cd /g/data/r78/ext547/Output/FDC/\n",
    "plt.savefig('FDC_'+gauge_of_interest+'_PE'+str(perexc_title2)+'.jpg') ## User should format file name, if required"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creation of image showing a small area directly around stream gauge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Prepare imagery\n",
    "rgb = nbar_clean.sel(time =time_slice_actual, method = 'nearest').to_array(dim='color').sel(color=['swir1','nir', 'green']).transpose('y', 'x', 'color')\n",
    "fake_saturation = 6000.0\n",
    "rgb = rgb.astype('double')\n",
    "clipped_visible = rgb.where(rgb<fake_saturation).fillna(fake_saturation)\n",
    "max_val = clipped_visible.max(['y', 'x'])\n",
    "scaled = (clipped_visible / max_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Create small area image for the event of interest\n",
    "\n",
    "fig = plt.figure(figsize =(8,8)) #Edit size of plot ##User should format as required\n",
    "plt.subplots_adjust(left=0.05, right=0.95, top=0.95, bottom=0.05) #Set border dimensions  ##User should format if required\n",
    "fig.patch.set_facecolor('white') #Make border white ##User should format if required\n",
    "fig.patch.set_alpha(0.99)#Make border white ##User should format if required\n",
    "plt.axis('off')#remove axis ##User should delete code if required\n",
    "\n",
    "#Edit plot title ##User should format as required\n",
    "plt.title('Date: '+str(time_slice_actual)[0:-9]  + \n",
    "          '    Discharge: ' + discharge_title2+' $m^3$ $day^{-1}$' +\n",
    "          '   Percentage exceedance: '+ str(perexc_title2) + '%', size=12) \n",
    "\n",
    "#Add marker to show location of stream gauge \n",
    "plt.scatter(x = [sg_x], y = [sg_y], c= 'r', marker = 'o', s=150)\n",
    "plt.imshow(scaled, interpolation = 'nearest',\n",
    "           extent=[scaled.coords['x'].min(), scaled.coords['x'].max(), \n",
    "                  scaled.coords['y'].min(), scaled.coords['y'].max()])\n",
    "plt.show() #Create plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Save figure\n",
    "##User modification: edit directory to save figure\n",
    "%cd /g/data/r78/ext547/Output/FDC/ \n",
    "plt.savefig('FDC_'+gauge_of_interest+'_PE'+ str(perexc_title2) + '_D'+str(discharge_title2) +\n",
    "            '_'+ str(time_slice_actual)[0:-9] +'_small'+ '.jpg') ##User should format file name if required"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creation of image showing a large area around the stream gauge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Create Datacube query for event of interest\n",
    "\n",
    "#Define date for event of interest\n",
    "start_of_epoch = time_slice_t1.strftime(\"%Y %m, %d\") \n",
    "end_of_epoch = time_slice_t2.strftime(\"%Y %m, %d\")\n",
    "\n",
    "#Define area of interest \n",
    "###User modification: change area of interest to suit gauge location in relation to satellite pass\n",
    "lat_max = sg_lat+ 0.6 #up\n",
    "lat_min = sg_lat- 0.6 #down\n",
    "lon_max = sg_lon+ 0.8 #right\n",
    "lon_min = sg_lon- 0.4 #left\n",
    "\n",
    "#Create query \n",
    "query2 = {   'time': (start_of_epoch, end_of_epoch)       }\n",
    "query2['x'] = (lon_min, lon_max)\n",
    "query2['y'] = (lat_max, lat_min)\n",
    "query2['crs'] = 'EPSG:4326'\n",
    "print (query2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Load image data for event of interest   \n",
    "image_of_interest = dc.load(product= satellite_type, group_by='solar_day',\n",
    "                            measurements = bands_of_interest,  **query2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Prepare imagery\n",
    "rgb2 = image_of_interest.to_array(dim='color').sel(color=['swir1',\n",
    "                                                          'nir', 'green']).squeeze().transpose('y', 'x', 'color')\n",
    "fake_saturation = 6000\n",
    "clipped_visible = rgb2.where(rgb2<fake_saturation).fillna(fake_saturation)\n",
    "max_val = clipped_visible.max(['y', 'x'])\n",
    "scaled = (clipped_visible / max_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Create large area image\n",
    "\n",
    "fig = plt.figure(figsize =(21,21))  #Edit size of plot ##User should format as required\n",
    "plt.subplots_adjust(left=0.05, right=0.95, top=0.95, bottom=0.05) #Set border dimensions  ##User should format if required\n",
    "fig.patch.set_facecolor('white') #Make border white ##User should format if required\n",
    "fig.patch.set_alpha(0.99) #Make border white ##User should format if required\n",
    "plt.axis('off') #remove axis ##User should delete code if required\n",
    "\n",
    "#Edit plot title ##User should format as required\n",
    "plt.title('Gauge: '+gauge_of_interest +'    Date: '+str(time_slice_actual)[0:-9]  + \n",
    "          '    Discharge: ' + discharge_title2+' $m^3$ $day^{-1}$' +\n",
    "          '   Percentage exceedance: '+ str(perexc_title2) + '%', size=22)\n",
    "\n",
    "#Add marker to show location of stream gauge \n",
    "plt.scatter(x = [sg_x], y = [sg_y], c= 'r', marker = 'o', s=500)\n",
    "plt.imshow(scaled, interpolation = 'nearest', \n",
    "           extent=[scaled.coords['x'].min(), scaled.coords['x'].max(),\n",
    "                   scaled.coords['y'].min(), scaled.coords['y'].max()])\n",
    "plt.show() #Create plot\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#save figure\n",
    "###User modification: edit directory to save figure\n",
    "%cd /g/data/r78/ext547/Output/FDC/ \n",
    "plt.savefig('FDC_'+gauge_of_interest+'_PE'+ str(perexc_title2) + '_D'+str(discharge_title2)+\n",
    "            '_'+ str(time_slice_actual)[0:-9] +'_large'+ '.jpg') ##User should format file name, if required"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stop here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# If required, save as image in netcdf format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Retrieve the original nbar dataset attributes (crs)\n",
    "\n",
    "attrs = image_of_interest #set up variable attributes to hold the attributes\n",
    "\n",
    "#get the band information\n",
    "bands = attrs.data_vars.keys()\n",
    "print (bands)\n",
    "for i in bands:\n",
    "    #drop band data, retaining just the attributes\n",
    "    attrs =attrs.drop(i)\n",
    "    \n",
    "#set up new variable called ndvi_var, and assign attributes to it in a dictionary\n",
    "image_var = {'scaled':''}\n",
    "image_output = attrs.assign(**image_var)\n",
    "image_output['scaled'] = scaled\n",
    "print (image_output)\n",
    "image_output2 = image_output.scaled.to_dataset(dim='color')\n",
    "\n",
    "#print image output\n",
    "image_output2.attrs['crs'] = image_output.crs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Create netcdf\n",
    "###User modification: edit directory and file name\n",
    "outfile = '/g/data/r78/ext547/Output/netcdf/'+ str(time_slice_actual)[0:-9] +'.nc' \n",
    "write_dataset_to_netcdf(image_output2,  variable_params={'scaled': {'zlib':True}}, filename=outfile) #Create file\n",
    "\n",
    "print ('wrote: '+outfile+' to netcdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  },
  "toc": {
   "toc_cell": false,
   "toc_number_sections": false,
   "toc_threshold": 6,
   "toc_window_display": true
  },
  "widgets": {
   "state": {
    "6dc82a33fe67407293537fd5b961062a": {
     "views": [
      {
       "cell_index": 25
      }
     ]
    }
   },
   "version": "1.2.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
